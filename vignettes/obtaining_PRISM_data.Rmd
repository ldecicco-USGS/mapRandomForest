---
title: "MAP: Obtaining PRISM data"
author: "Steve Westenbroek"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MAP: Obtaining PRISM data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This example shows how the functions modeled after Brian Breaker's scripts may be used to recreate the data files used in his random forest model for the Mississippi Alluvial Plain (MAP) project area. The model is referred to hereafter as the 'RFMAP' model.

## Initialize environment and read/reproject shapefile

It is more efficient to read and reproject the shapefile once; we'll read in the shapefile and an example raster containing PRISM data, then reproject the shapefile so that it is in the same projection as the rasters.

```{r Generate_site_list}
library(mapRandomForest)
library(raster)
library(foreach)
library(lubridate)
library(stringr)
library(readr)
library(dplyr)
library(data.table)

# The monthly PRISM dataset is many gigabytes of data; download this locally and change the
# path descriptor as appropriate if running the vignettes locally
#path_to_PRISM_data <- "/Volumes/Samsung_T5/Users/SMWData/Weather_and_GIS/PRISM_Monthly_4k"
path_to_PRISM_data <- "/home/nobody/geodata_and_weather_data/PRISM_Monthly_4k"

test_PRISM_file <- paste(path_to_PRISM_data,"ppt","PRISM_ppt_stable_4kmM2_1896_bil.bil",sep="/")
test_PRISM_raster <- raster::raster(test_PRISM_file)

# read in previously calculated monthly discharge values
flow_data <- readr::read_csv('monthly_discharge.csv')

# we currently have no PRISM data for 2018; must subset date values
# set day to middle of month; parse as date; filter
flow_data$yyyymmdd <- lubridate::ymd(paste(flow_data$date,"15",sep="-"))

# eliminate data from 2018, mean values calculated from less than 25 values, or values
# associated with regulated flow or significant flow diversions
flow_data <- flow_data %>% 
               dplyr::filter(yyyymmdd < lubridate::ymd("2018-01-01")) %>%
               dplyr::filter(count_non_na > 24 ) %>%
               dplyr::filter(!is_regulated)

# read in shapefile; sites are identified by the attribute 'site_no'
shapefile_name <- system.file("extdata","sitesAll.shp", package = "mapRandomForest")
sites_sp       <- raster::shapefile(shapefile_name)

sites_reprojected_sp <- sp::spTransform(sites_sp, CRSobj = test_PRISM_raster@crs)

```


Next, we create a wrapper function around the package function `getRasterMean`, in order to help in parallelizing the calculation.

```{r}

getRasterMean_wrapper <- function(site_no, month, year, dataname='ppt') {
  
  # during parallel operations, anything written to std_out is lost; write to 
  # a logile instead so we have some idea of the progress being made
  write(file="logfile.txt", append=TRUE, x=paste(site_no, month, year,dataname))
  
  path <- paste(path_to_PRISM_data,'/',dataname,'/',sep="")
  raster_filename <- list.files(path, 
                                pattern=glob2rx(paste("*",dataname,"*stable*",as.character(year),
                                                as.character(month),"*.bil",sep="")),
                                full.names=TRUE)
  
  shp <- sites_reprojected_sp[sites_reprojected_sp@data$site_no==site_no, ]
  
  # crude error trapping in the event that a particular PRISM image file cannot be found
  if (length(raster_filename) > 0 ) {
    if (file.exists( raster_filename) ) {
      value <- getRasterMean(raster_filename, shp)
    } else {
      write(file="logfile.txt", append=TRUE, paste("**ERROR** - file doesn't exist:", raster_filename))
      value <- NA
    }
  } else {
    write(file="logfile.txt", append=TRUE, paste("**ERROR** - file not found:", raster_filename))
    value <- NA
  }  
  
  return(as.numeric(value))
  
}


```


```{r, eval=FALSE}

# set up 7 workers
cl <- parallel::makeCluster(7)

# must register parallel backend with the 'foreach' package
doParallel::registerDoParallel(cl)


site_numbers <- unique(sites_reprojected_sp$site_no)
dataname <- "ppt"

file.remove("logfile.txt")

mean_ppt_values <- foreach::foreach(b=site_numbers, .combine=rbind) %:%

  foreach::foreach(a=dplyr::filter(flow_data, site_no==b)$date,.combine=rbind,
                                        .packages=c("sp","dplyr","raster","stringr",
                                          "data.table","lubridate","velox","mapRandomForest")) %dopar% {
    
    value <- getRasterMean_wrapper(site_no=b, 
                                   month=unlist(stringr::str_split(a,"-"))[2],
                                   year=unlist(stringr::str_split(a,"-"))[1],
                                   dataname=dataname )

    data.frame(site_no=b,date=a,value=value)
  }

parallel::stopCluster(cl)


```
