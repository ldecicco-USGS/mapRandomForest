---
title: "MAP: Obtaining streamflow data"
author: "Steve Westenbroek"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MAP: Obtaining streamflow data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This example shows how the functions modeled after Brian Breaker's scripts may be used to recreate the data files used in his random forest model for the Mississippi Alluvial Plain (MAP) project area. The model is referred to hereafter as the 'RFMAP' model.

## Create a list of stream gages

The list of stream gages used in RFMAP was created by importing a shapefile and extracting the unique site numbers (gage IDs) from the file. The shapefile contains polygons defining the drainage basins for each of the USGS gages used to train the RFMAP. The attribute table simply contains the USGS site number and the drainage basin size in square miles.

```{r Generate_site_list}
library(mapRandomForest)
library(raster)
library(doParallel)
library(foreach)
library(readr)
library(dplyr)
library(data.table)

# read in shapefile; sites are identified by the attribute 'site_no'
shapefile_name <- system.file("extdata","sitesAll.shp", package = "mapRandomForest")
sites_sp       <- raster::shapefile(shapefile_name)

site_list <- unique(sites_sp$site_no)


```
```{r, echo=FALSE}
site_list
```

## Pull USGS gage data for the entire list of sites, summarizing by month

The process of obtaining and summarizing USGS gage data for RFMAP can be broken into the following steps:

1) Use the package 'dataRetrieval' to obtain site information and daily discharge data for the period of record
2) Temporarily fill in missing values with the number 1; this is to 'fake out' the baseflow separation routine called in the next step
3) Use the 'part' routine from package 'DVstats' to compute a baseflow separation on the daily discharge values
4) Filter (delete) all records which originally contained missing values
5) Compute the mean discharge by month for the months that have enough data to be processed
6) Pull USGS peak flow file data for gage; delete months that fall within range of 
   dates marked as "regulated" or subect to "diversions" (peak flow 'peak_cd' 5, 6, or C)

The 6 steps listed above are encapsulated in the function `getFlowDataForSite`.


```{r Pull_USGS_data, eval=FALSE}
library(data.table)
#monthly_discharge <- data.table(site_no = as.character(), date = as.character(),
#                                discharge = as.numeric(), baseflow = as.numeric(),
#                                count_non_na=as.numeric(), is_regulated = as.logical())

for (site in site_list[1:3]) {
  print(paste("Processing site: ",site,sep=""))
  data_list <- getFlowDataForSite(site)
  dt <- data.table(site_no=rep(site,nrow(data_list$df)), date=data_list$df$date,
                   discharge=data_list$df$discharge,baseflow=data_list$df$baseflow,
                   count_non_na=data_list$df$count_non_na, is_regulated=data_list$df$is_regulated)
  monthly_discharge <- rbind(monthly_discharge, dt)
  
}

# eliminate any monthly mean values calculated for months with less than 24 values
monthly_discharge <- dplyr::filter(monthly_discharge, monthly_discharge$count_non_na > 24)

# write results to a 'csv' file - not run - don't want to overwrite the all-sites file
#readr::write_csv(monthly_discharge, "monthly_discharge.csv")

```
### Parallel version to obtain *all* gage data (not run)

The following code will fire up 5 local workers and download all discharge data for the sites contained in the `sitesAll.shp` shapefile, summarized by month. This code snippet ran in about 5 minutes on a 2012 MacBook Pro, using 5 cores.

```{r Parallel_pull_USGS_data, eval=FALSE}

site_numbers <- unique(sites_sp$site_no)

# set up 5 workers
cl <- parallel::makeCluster(5)

# must register parallel backend with the 'foreach' package
doParallel::registerDoParallel(cl)

monthly_discharge <- foreach::foreach(b=site_numbers, .combine=rbind,
                                      .packages=c("sp","dplyr","dataRetrieval","stringr",
                                        "data.table","lubridate","DVstats","mapRandomForest"),
                                       .errorhandling='remove') %dopar% {
  
  print(b)                                          
  data_list <- mapRandomForest::getFlowDataForSite(b)
  data.table(site_no=rep(b,nrow(data_list$df)), date=data_list$df$date,
                   discharge=data_list$df$discharge,baseflow=data_list$df$baseflow,
                   count_non_na=data_list$df$count_non_na, is_regulated=data_list$df$is_regulated)
}

# eliminate weird negative discharge values and summaries based on less than 25 daily values per month
monthly_discharge <- monthly_discharge %>% 
                       dplyr::filter(count_non_na > 24) %>%
                       dplyr::filter(discharge >= 0)

# write results to a 'csv' file
readr::write_csv(monthly_discharge, "monthly_discharge.csv")

parallel::stopCluster(cl)
                                     
```

## Compare to Brian Breaker's original file.

```{r, Compare_Breaker_Q}

monthly_discharge_orig <- readr::read_csv("dvsMonthly.csv")
monthly_discharge_orig <- monthly_discharge_orig %>% 
                            dplyr::filter(!is.na(Flow)) %>%
                            dplyr::filter(Flow >= 0 )

# Brian's `createModels.R` script contained a list of sites to remove
removeSites <- c("03320500","03383000","03434500","03436100","03438000","03438220", 
                 "03588000","03601990","03602500","03603000","03604400","03605555", 
                 "03436690","03436700","03584000","03584020","03588400","03588500", 
                 "03599450","03600500","03601630","03604000","03433641")

monthly_discharge_orig <- monthly_discharge_orig %>% dplyr::filter(!siteNo %in% removeSites)


monthly_discharge_vignette <- readr::read_csv("monthly_discharge.csv")
monthly_discharge_vignette$yyyymmdd <- lubridate::ymd(paste(monthly_discharge_vignette$date,"15",sep="-"))

# clip off the more recent data to make comparisons to Brian's original data
monthly_discharge_vignette <- monthly_discharge_vignette %>%
                                dplyr::filter(yyyymmdd < lubridate::ymd("2016-10-31")) %>%
                                dplyr::filter(!is_regulated)


sites_orig <- sort(unique(monthly_discharge_orig$siteNo))
sites <- sort(unique(monthly_discharge_vignette$site_no))

monthly_discharge_orig$joinfield <- with(monthly_discharge_orig, paste(siteNo,Date,sep="_"))
monthly_discharge_vignette$joinfield <- with(monthly_discharge_vignette, paste(site_no,date,sep="_"))

comparison_df <- dplyr::left_join(x=monthly_discharge_orig, y=monthly_discharge_vignette,
                                  by=("joinfield"="joinfield"))
comparison_df$site_no <- as.factor(comparison_df$site_no)

missing_from_current_df <- comparison_df %>% dplyr::filter(is.na(discharge))
```

Approximately 777 streamflow records (out of about 147,000) in the original dataset are either unobtainable because of public availibility settings in NWIS Wes, or are excluded in the current set of scripts owing to a stricter interpretation of the peak flow file codes indicating highly regulated or channelized flow conditions.
