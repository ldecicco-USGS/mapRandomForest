---
title: "Part 3: Summarize PRISM data"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_document:
    toc: true
    toc_depth: 2
    number_sections: true
vignette: >
  %\VignetteIndexEntry{3. Summarize PRISM data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this vignette we process the PRISM climate data, calculating zonal statistics for precipitation and temperature values for each month for the drainage areas associated with the USGS gages. The code in this vignette assumes that an earlier vignette has been run and that the file `monthly_discharge.csv.gz` has been generated in that step.

# Initialize environment and read/reproject shapefile

We read in a single PRISM raster to extract its PROJ4 string; this is used to reproject the shapefile containing the drainage areas for the gage sites into the same coordinate reference system as the PRISM data.

```{r Generate_site_list}
# the 'magrittr' package provides the 'pipe' operator ('%>%') used throughout the vignettes
library(magrittr)

# The monthly PRISM dataset is many gigabytes of data; download this locally and change the
# path descriptor as appropriate if running the vignettes locally
path_to_PRISM_data <- "g:\\PRISM_Monthly_4k"

test_PRISM_file   <- paste(path_to_PRISM_data,"ppt","PRISM_ppt_stable_4kmM2_1896_bil.bil",sep="/")
test_PRISM_raster <- velox::velox(test_PRISM_file)

# read in shapefile; sites are identified by the attribute 'site_no'
shapefile_name <- system.file("data","sitesAll.shp", package = "mapRandomForest")
sites_sp       <- raster::shapefile(shapefile_name)

# reproject gage site shapefile to align with the projection used with the PRISM data files
sites_reprojected_sp <- sp::spTransform(sites_sp, CRSobj = test_PRISM_raster$crs)

# maximum number of "clusters" to create for parallel processing; usually 5-10 on a laptop
max_clusters      <- 10
```

Read in previously calculated discharge data, making sure that we have a date we can use later on in the processing. Note also that we're filtering discharge values to exclude:

* discharge values from the year 2018 or later
* monthly values calculated for a month with less than 25 missing values
* monthly values calculated for gages on streams subject to significant regulation

```{r read in previously calculated values}
flow_data_name <- system.file("data","monthly_discharge.rds", package = "mapRandomForest")
# read in previously calculated monthly discharge values
flow_data <- readr::read_rds(flow_data_name)

# we currently have no PRISM data for 2018; must subset date values
# set day to middle of month; parse as date; filter
flow_data$yyyymmdd <- lubridate::ymd(paste(flow_data$date,"01",sep="-"))

# eliminate data from 2018, mean values calculated from less than 25 values, or values
# associated with regulated flow or significant flow diversions
flow_data <- flow_data %>% 
               dplyr::filter(yyyymmdd < lubridate::ymd("2018-01-01")) %>%
               dplyr::filter(count_non_na > 24 ) %>%
               dplyr::filter(!is_regulated)

```

Next, we create a wrapper function around the package function `getRasterMean`, in order to help in parallelizing the calculation. The function accepts a value for `dataname`, which is a character representation of the label used to name the PRISM files ('ppt', 'tmin', 'tmax', or 'tmean'). The function sifts through the PRISM data directory, finds the full name of the file associated with the combination of month/year/dataname, and calculates zonal statistics for the drainage area associated with the supplied site number.

```{r}

getRasterMean_wrapper <- function(site_no, month, year, dataname='ppt') {
  
  # during parallel operations, anything written to std_out is lost; write to 
  # a logile instead so we have some idea of the progress being made
  write(file="logfile.txt", append=TRUE, 
        x=paste(system("hostname"),site_no, month, year,dataname))
  
  path <- paste(path_to_PRISM_data,'/',dataname,'/',sep="")
  raster_filename <- list.files(path, 
                                pattern=glob2rx(paste("*",dataname,"*stable*",as.character(year),
                                                as.character(month),"*.bil",sep="")),
                                full.names=TRUE)
  
  shp <- sites_reprojected_sp[sites_reprojected_sp@data$site_no==site_no, ]
  
  # crude error trapping in the event that a particular PRISM image file cannot be found
  if (length(raster_filename) > 0 ) {
    if (file.exists( raster_filename) ) {
      value <- getRasterMean(raster_filename, shp)
    } else {
      write(file="logfile.txt", append=TRUE, paste("**ERROR** - file doesn't exist:", raster_filename))
      value <- NA
    }
  } else {
    write(file="logfile.txt", append=TRUE, paste("**ERROR** - file not found:", raster_filename))
    value <- NA
  }  
  
  return(as.numeric(value))
  
}
```

# Set up parallel computing environment

```{r setup_parallel_computing_environment, eval=FALSE}

cl <- parallel::makeCluster(max_clusters)

# must register parallel backend with the 'foreach' package
doParallel::registerDoParallel(cl)
site_numbers <- unique(sites_reprojected_sp$site_no)
```

# Summarize mean PRISM precipitation values

```{r summarize_precipitation, eval=FALSE}
dataname <- "ppt"

file.remove("logfile.txt")

mean_precip_values <- foreach::foreach(b=site_numbers, .combine=rbind) %:%

  foreach::foreach(a=dplyr::filter(flow_data, site_no==b)$date,.combine=rbind,
                                        .packages=c("magrittr","dplyr","raster","stringr",
                                          "data.table","lubridate","velox","mapRandomForest")) %dopar% {
    
    value <- getRasterMean_wrapper(site_no=b, 
                                   month=unlist(stringr::str_split(a,"-"))[2],
                                   year=unlist(stringr::str_split(a,"-"))[1],
                                   dataname=dataname )

    data.frame(site_no=b,date=a,value=value)
                                          }
readr::write_rds(mean_precip_values,"mean_precip_values.rds")
```

# Summarize mean PRISM minimum air temperature values

```{r summarize_minimum_air_temps, eval=FALSE}
dataname <- "tmin"

mean_tmin_values <- foreach::foreach(b=site_numbers, .combine=rbind) %:%

  foreach::foreach(a=dplyr::filter(flow_data, site_no==b)$date,.combine=rbind,
                                        .packages=c("magrittr","dplyr","raster","stringr",
                                          "data.table","lubridate","velox","mapRandomForest")) %dopar% {
    
    value <- getRasterMean_wrapper(site_no=b, 
                                   month=unlist(stringr::str_split(a,"-"))[2],
                                   year=unlist(stringr::str_split(a,"-"))[1],
                                   dataname=dataname )

    data.frame(site_no=b,date=a,value=value)
                                          }
readr::write_rds(mean_tmin_values,"mean_tmin_values.rds")
```

# Summarize mean PRISM maximum air temperature values

```{r summarize_max_air_temps, eval=FALSE}
dataname <- "tmax"

mean_tmax_values <- foreach::foreach(b=site_numbers, .combine=rbind) %:%

  foreach::foreach(a=dplyr::filter(flow_data, site_no==b)$date,.combine=rbind,
                                        .packages=c("magrittr","dplyr","raster","stringr",
                                          "data.table","lubridate","velox","mapRandomForest")) %dopar% {
    
    value <- getRasterMean_wrapper(site_no=b, 
                                   month=unlist(stringr::str_split(a,"-"))[2],
                                   year=unlist(stringr::str_split(a,"-"))[1],
                                   dataname=dataname )

    data.frame(site_no=b,date=a,value=value)
  }

readr::write_rds(mean_tmax_values,"mean_tmax_values.rds")
```


# Summarize mean PRISM mean air temperature values

```{r, summarize_mean_air_temps, eval=FALSE}
dataname <- "tmean"

mean_tmean_values <- foreach::foreach(b=site_numbers, .combine=rbind) %:%

  foreach::foreach(a=dplyr::filter(flow_data, site_no==b)$date,.combine=rbind,
                                        .packages=c("magrittr","dplyr","raster","stringr",
                                          "data.table","lubridate","velox","mapRandomForest")) %dopar% {
    
    value <- getRasterMean_wrapper(site_no=b, 
                                   month=unlist(stringr::str_split(a,"-"))[2],
                                   year=unlist(stringr::str_split(a,"-"))[1],
                                   dataname=dataname )

    data.frame(site_no=b,date=a,value=value)
  }

readr::write_rds(mean_tmean_values,"mean_tmean_values.rds")
```


```{r stop_cluster, eval=FALSE}
parallel::stopCluster(cl)
```

# Read in summarized values

Because the preceding steps are compute-intensive, the results have been saved as a set of *.rds files. We now read those back in. In subsequent steps we will calculated lagged values. We need a way to identify blocks of continuous data associated with each USGS gage ID. At breaks in the dataset for each gage ID, the lag calculation must be reset. This is accomplished by assigning a `block_id` to each contiguous (in time) block of data; at a break in the date value, the `block_id` is imcremented by one.

```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <-"
| site_no  | date    | precip | precip_lag_1m | precip_lag_2m |    block_id  |
|----------|---------|--------|---------------|---------------|--------------|
| 07024000 | 1940-01 |  155.  |     NA        |       NA      |       1      |
| 07024000 | 1940-02 |  210.  |     155.      |       NA      |       1      |
| 07024000 | 1940-03 |  305.  |     210.      |       155.    |       1      |
| 07024000 | 1940-04 |  240.  |     305.      |       210.    |       1      |
| 07024000 | *1940-08* |  430.  |     NA        |       NA      |       2      |
| 07024000 | 1940-09 |  302.  |     430.      |       NA      |       2      |
| 07024000 | 1940-10 |  275.  |     302.      |       430.    |       2      |
| 07024000 | 1940-11 |  210.  |     275.      |       302.    |       2      |
"
cat(tabl)
```


  
```{r Read_in_csv_values}
library(mapRandomForest)

tmin_values_filename <- system.file("data","mean_tmin_values.rds", package = "mapRandomForest")
mean_tmin_values <- readr::read_rds(tmin_values_filename) %>%
                           dplyr::group_by(site_no) %>%
                           dplyr::mutate(block_id=identify_contiguous(date)) %>%
                           dplyr::mutate(group_id=paste(site_no,block_id,sep="_"))

tmax_values_filename <- system.file("data","mean_tmin_values.rds", package = "mapRandomForest")
mean_tmax_values <- readr::read_rds(tmax_values_filename) %>%
                           dplyr::group_by(site_no) %>%
                           dplyr::mutate(block_id=identify_contiguous(date)) %>%
                           dplyr::mutate(group_id=paste(site_no,block_id,sep="_"))
tmean_values_filename <- system.file("data","mean_tmin_values.rds", package = "mapRandomForest")
mean_tmean_values <- readr::read_rds(tmean_values_filename) %>%
                           dplyr::group_by(site_no) %>%
                           dplyr::mutate(block_id=identify_contiguous(date)) %>%
                           dplyr::mutate(group_id=paste(site_no,block_id,sep="_"))
precip_values_filename <- system.file("data","mean_precip_values.rds", package = "mapRandomForest")
mean_precip_values <- readr::read_rds(precip_values_filename) %>% 
                     dplyr::group_by(site_no) %>%
                     dplyr::mutate(block_id=identify_contiguous(date)) %>%
                     dplyr::mutate(group_id=paste(site_no,block_id,sep="_"))
```


# Create lagged values for precipitation data

```{r create_lagged_values_for_precip}
mean_precip_values_lagged <- mean_precip_values %>%
                     dplyr::mutate(precip=value) %>%
                     dplyr::select(-c(value)) %>%
                     dplyr::group_by(group_id) %>%
                     dplyr::mutate(precip_lag_1m=dplyr::lag(precip,n=1)) %>%
                     dplyr::mutate(precip_lag_2m=dplyr::lag(precip,n=2)) %>%
                     dplyr::mutate(precip_lag_3m=dplyr::lag(precip,n=3)) %>%
                     dplyr::mutate(precip_lag_4m=dplyr::lag(precip,n=4)) %>%
                     dplyr::mutate(precip_lag_5m=dplyr::lag(precip,n=5)) %>%
                     dplyr::mutate(precip_lag_6m=dplyr::lag(precip,n=6)) %>%
                     dplyr::mutate(precip_sum_6m=precip_lag_1m +
                                                 precip_lag_2m +
                                                 precip_lag_3m +
                                                 precip_lag_4m +
                                                 precip_lag_5m +
                                                 precip_lag_6m )

mean_precip_values_lagged <- mean_precip_values_lagged %>%
                            dplyr::filter(!is.na(precip_sum_6m))

readr::write_rds(mean_precip_values_lagged, "mean_precip_values_w_lag.rds", compress="gz")
```


# Create lagged values for air temperature data

The code to process air temperature data values performs the following steps:

* create a named variable to hold specific data (e.g. `precip` rather than `value`)
* eliminate the generic `value` field
* group the table by `group_id`, a combinination of the USGS gage ID and the contiguous block ID
* calculate the lagged values
* eliminate rows with NA values in the calculated lag fields

## Create lagged values for minimum air temperature data

```{r create_lagged_values_for_minimum_air_temperature}
mean_tmin_values_lagged <- mean_tmin_values %>%
                     dplyr::mutate(tmin=value) %>%
                     dplyr::select(-c(value)) %>%
                     dplyr::group_by(group_id) %>%
                     dplyr::mutate(tmin_lag_1m=dplyr::lag(tmin,n=1,default=NA)) %>%
                     dplyr::mutate(tmin_lag_2m=dplyr::lag(tmin,n=2,default=NA)) %>%
                     dplyr::ungroup() %>%
                     dplyr::filter(!is.na(tmin_lag_1m)) %>%
                     dplyr::filter(!is.na(tmin_lag_2m))

readr::write_rds(mean_tmin_values_lagged, "mean_tmin_values_w_lag.rds")
```

## Create lagged values for maximum air temperature data

```{r create_lagged_values_for_maximum_air_temperature}
mean_tmax_values_lagged <- mean_tmax_values %>%
                     dplyr::mutate(tmax=value) %>%
                     dplyr::select(-c(value)) %>%
                     dplyr::group_by(group_id) %>%
                     dplyr::mutate(tmax_lag_1m=dplyr::lag(tmax,n=1,default=NA)) %>%
                     dplyr::mutate(tmax_lag_2m=dplyr::lag(tmax,n=2,default=NA)) %>%
                     dplyr::ungroup() %>%
                     dplyr::filter(!is.na(tmax_lag_1m)) %>%
                     dplyr::filter(!is.na(tmax_lag_2m))

readr::write_rds(mean_tmax_values_lagged, "mean_tmax_values_w_lag.rds")
```

## Create lagged values for mean air temperature data

```{r create_lagged_values_for_mean_air_temperature}
mean_tmean_values_lagged <- mean_tmean_values %>%
                     dplyr::mutate(tmean=value) %>%
                     dplyr::select(-c(value)) %>%
                     dplyr::group_by(group_id) %>%
                     dplyr::mutate(tmean_lag_1m=dplyr::lag(tmean,n=1,default=NA)) %>%
                     dplyr::mutate(tmean_lag_2m=dplyr::lag(tmean,n=2,default=NA)) %>%
                     dplyr::ungroup() %>%
                     dplyr::filter(!is.na(tmean_lag_1m)) %>%
                     dplyr::filter(!is.na(tmean_lag_2m))

readr::write_rds(mean_tmean_values_lagged, "mean_tmean_values_w_lag.rds")
```


